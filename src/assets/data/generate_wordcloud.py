#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯äº‘ç”Ÿæˆè„šæœ¬
ä»æ­Œå•JSONæ–‡ä»¶ä¸­æå–lyricså’Œcommentsï¼Œç”Ÿæˆè¯äº‘å›¾
"""

import json
import re
import os
from collections import Counter

# å®‰è£…ä¾èµ–ï¼špip install jieba wordcloud matplotlib --break-system-packages

import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# ==================== é…ç½®åŒº ====================

# JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
JSON_FILES = [
    '/mnt/user-data/uploads/new_playlists_data.json',
    '/mnt/user-data/uploads/æ­Œå•_2314343014.json',
    '/mnt/user-data/uploads/æ­Œå•_4928935213.json',
    '/mnt/user-data/uploads/æ­Œå•_5151662311.json',
    '/mnt/user-data/uploads/æ­Œå•_17562030729.json',
]

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
OUTPUT_PATH = '/mnt/user-data/outputs/è¯äº‘å›¾.png'

# ä¸­æ–‡å­—ä½“è·¯å¾„ï¼ˆè¯äº‘éœ€è¦ä¸­æ–‡å­—ä½“ï¼‰
FONT_PATH = '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc'

# åœç”¨è¯åˆ—è¡¨ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
STOP_WORDS = set([
    # å¸¸è§åœç”¨è¯
    'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'å°±',
    'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'ä¹Ÿ', 'éƒ½', 'ä¼š', 'èƒ½', 'å¯ä»¥',
    'ä¸', 'æ²¡', 'æ²¡æœ‰', 'å¾ˆ', 'å¤ª', 'æ›´', 'æœ€', 'æŠŠ', 'è¢«', 'è®©', 'ç»™', 'åˆ°',
    'å»', 'æ¥', 'ä¸Š', 'ä¸‹', 'ä¸­', 'é‡Œ', 'å¤–', 'å‰', 'å', 'å·¦', 'å³', 'ä¸ª',
    'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ª', 'å“ªé‡Œ', 'è°', 'å¤šå°‘', 'å‡ ', 'å¦‚ä½•',
    'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'è‡ªå·±', 'ä»€ä¹ˆ', 'ä¸€ä¸ª', 'ä¸€äº›', 'ä¸€æ ·',
    'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ä½†æ˜¯', 'ç„¶å', 'è¿˜æ˜¯', 'æˆ–è€…', 'è€Œä¸”',
    'ä¸æ˜¯', 'å°±æ˜¯', 'åªæ˜¯', 'è¿˜æœ‰', 'å·²ç»', 'ä¸€ç›´', 'ä¸€å®š', 'ä¸€èµ·', 'ä¸€ä¸‹',
    'å•Š', 'å§', 'å‘¢', 'å—', 'å“¦', 'å“ˆ', 'å—¯', 'å‘€', 'å“', 'å”‰', 'å˜¿', 'å–‚',
    'çœŸçš„', 'çœŸæ˜¯', 'ç¡®å®', 'å…¶å®', 'å¯èƒ½', 'åº”è¯¥', 'éœ€è¦', 'æƒ³è¦', 'çŸ¥é“',
    'è§‰å¾—', 'æ„Ÿè§‰', 'çœ‹åˆ°', 'å¬åˆ°', 'è¯´', 'æƒ³', 'çœ‹', 'å¬', 'åš', 'èµ°', 'è·‘',
    # æ­Œè¯ä¸­å¸¸è§æ— æ„ä¹‰è¯
    'ä½œè¯', 'ä½œæ›²', 'ç¼–æ›²', 'åˆ¶ä½œäºº', 'æ··éŸ³', 'æ¯å¸¦', 'æ¼”å”±', 'åŸå”±',
    'feat', 'prod', 'remix', 'live', 'cover', 'version',
    # è‹±æ–‡å¸¸è§è¯
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'can', 'need', 'shall',
    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
    'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours', 'ours',
    'this', 'that', 'these', 'those', 'what', 'which', 'who', 'whom', 'whose',
    'and', 'or', 'but', 'if', 'because', 'as', 'when', 'while', 'although',
    'to', 'of', 'in', 'on', 'at', 'by', 'for', 'with', 'about', 'from',
    'into', 'through', 'during', 'before', 'after', 'above', 'below',
    'up', 'down', 'out', 'off', 'over', 'under', 'again', 'further', 'then',
    'so', 'than', 'too', 'very', 'just', 'only', 'now', 'here', 'there',
    'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other', 'some',
    'such', 'no', 'not', 'any', 'same', 'different', 'own',
    'yeah', 'oh', 'ah', 'uh', 'um', 'hmm', 'hey', 'yo', 'ya', 'yea', 'na',
    'la', 'da', 'di', 'do', 'de', 'le', 'lo', 'baby', 'babe', 'boy', 'girl',
    'like', 'dont', 'wanna', 'gonna', 'gotta', 'aint', 'cant', 'wont',
    'know', 'got', 'get', 'let', 'come', 'go', 'make', 'take', 'see', 'say',
    # ç‰¹æ®Šç¬¦å·å’Œæ•°å­—
    'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å', 'ç™¾', 'åƒ', 'ä¸‡',
])

# ==================== å‡½æ•°å®šä¹‰ ====================

def load_json_files(file_paths):
    """åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶"""
    all_data = []
    for path in file_paths:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_data.extend(data)
                print(f"âœ… åŠ è½½æˆåŠŸ: {os.path.basename(path)} ({len(data)} é¦–æ­Œ)")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {path} - {e}")
    return all_data


def extract_text(data):
    """ä»æ•°æ®ä¸­æå–lyricså’Œcommentsæ–‡æœ¬"""
    texts = []
    
    for song in data:
        # æå–æ­Œè¯
        lyric = song.get('lyric', '')
        if lyric:
            texts.append(lyric)
        
        # æå–è¯„è®º
        comments = song.get('comments', [])
        if comments:
            texts.extend(comments)
    
    return '\n'.join(texts)


def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z\s]', ' ', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    # è½¬å°å†™ï¼ˆè‹±æ–‡ï¼‰
    text = text.lower()
    return text


def segment_text(text):
    """ä½¿ç”¨jiebaåˆ†è¯"""
    words = jieba.cut(text)
    return list(words)


def filter_words(words, min_length=2):
    """è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯"""
    filtered = []
    for word in words:
        word = word.strip()
        # è¿‡æ»¤æ¡ä»¶ï¼šé•¿åº¦>=min_lengthï¼Œä¸åœ¨åœç”¨è¯åˆ—è¡¨ä¸­
        if len(word) >= min_length and word.lower() not in STOP_WORDS:
            filtered.append(word)
    return filtered


def generate_wordcloud(word_freq, output_path, font_path):
    """ç”Ÿæˆè¯äº‘å›¾"""
    wc = WordCloud(
        font_path=font_path,
        width=1600,
        height=1000,
        background_color='white',
        max_words=300,
        max_font_size=200,
        min_font_size=10,
        random_state=42,
        colormap='viridis',  # é…è‰²æ–¹æ¡ˆï¼Œå¯é€‰ï¼š'plasma', 'magma', 'inferno', 'cividis', 'Set2'
        prefer_horizontal=0.7,
    )
    
    wc.generate_from_frequencies(word_freq)
    
    # ä¿å­˜å›¾ç‰‡
    wc.to_file(output_path)
    print(f"âœ… è¯äº‘å·²ä¿å­˜è‡³: {output_path}")
    
    # æ˜¾ç¤ºè¯äº‘ï¼ˆå¯é€‰ï¼‰
    plt.figure(figsize=(16, 10))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()


def main():
    print("=" * 50)
    print("ğŸµ æ­Œå•è¯äº‘ç”Ÿæˆå™¨")
    print("=" * 50)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ æ­£åœ¨åŠ è½½JSONæ–‡ä»¶...")
    data = load_json_files(JSON_FILES)
    print(f"ğŸ“Š å…±åŠ è½½ {len(data)} é¦–æ­Œæ›²")
    
    # 2. æå–æ–‡æœ¬
    print("\nğŸ“ æ­£åœ¨æå–æ­Œè¯å’Œè¯„è®º...")
    raw_text = extract_text(data)
    print(f"ğŸ“Š æå–æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
    
    # 3. æ¸…ç†æ–‡æœ¬
    print("\nğŸ§¹ æ­£åœ¨æ¸…ç†æ–‡æœ¬...")
    clean = clean_text(raw_text)
    
    # 4. åˆ†è¯
    print("\nâœ‚ï¸ æ­£åœ¨åˆ†è¯...")
    words = segment_text(clean)
    print(f"ğŸ“Š åˆ†è¯ç»“æœ: {len(words)} ä¸ªè¯")
    
    # 5. è¿‡æ»¤åœç”¨è¯
    print("\nğŸ” æ­£åœ¨è¿‡æ»¤åœç”¨è¯...")
    filtered_words = filter_words(words)
    print(f"ğŸ“Š è¿‡æ»¤å: {len(filtered_words)} ä¸ªè¯")
    
    # 6. ç»Ÿè®¡è¯é¢‘
    print("\nğŸ“ˆ æ­£åœ¨ç»Ÿè®¡è¯é¢‘...")
    word_freq = Counter(filtered_words)
    print(f"ğŸ“Š ä¸åŒè¯æ±‡æ•°: {len(word_freq)}")
    print("\nğŸ” è¯é¢‘ Top 20:")
    for word, count in word_freq.most_common(20):
        print(f"   {word}: {count}")
    
    # 7. ç”Ÿæˆè¯äº‘
    print("\nğŸ¨ æ­£åœ¨ç”Ÿæˆè¯äº‘...")
    generate_wordcloud(dict(word_freq), OUTPUT_PATH, FONT_PATH)
    
    print("\n" + "=" * 50)
    print("âœ… å®Œæˆï¼")
    print("=" * 50)


if __name__ == '__main__':
    main()
